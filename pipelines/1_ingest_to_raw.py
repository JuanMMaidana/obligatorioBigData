#!/usr/bin/env python3
"""
Pipeline 1: Ingesta de datos crudos al Data Lake
===============================================

Este script implementa la primera etapa del pipeline ETL, donde se realiza la ingesta
de datos desde las fuentes originales (archivos CSV) hacia la zona RAW del Data Lake.

Zona RAW:
- Es una copia fiel y optimizada de la fuente original
- Sirve como backup permanente de los datos histÃ³ricos
- Formato optimizado (Parquet) para consultas posteriores mÃ¡s eficientes
- CompresiÃ³n snappy para reducir el espacio de almacenamiento
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp
from pyspark.sql.types import *
import os

def create_spark_session():
    """
    Crea una sesiÃ³n de Spark configurada para el procesamiento de datos.
    
    Returns:
        SparkSession: SesiÃ³n de Spark configurada
    """
    return SparkSession.builder \
        .appName("MovieDataLake-RawIngestion") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()

def ingest_movies_metadata(spark):
    """
    Ingesta los metadatos de pelÃ­culas desde CSV hacia la zona RAW.
    
    Args:
        spark (SparkSession): SesiÃ³n de Spark
    """
    print("ğŸ“½ï¸  Procesando metadatos de pelÃ­culas...")
    
    # Leer archivo CSV de metadatos de pelÃ­culas
    movies_df = spark.read.csv(
        "movies_metadata.csv",
        header=True,
        inferSchema=True,
        escape='"',
        multiLine=True
    )
    
    # AÃ±adir columna de fecha de ingesta para trazabilidad
    movies_df_with_timestamp = movies_df.withColumn(
        "ingestion_date", 
        current_timestamp()
    )
    
    # Guardar en zona RAW como Parquet con compresiÃ³n snappy
    output_path = "datalake/raw/movies_metadata/"
    movies_df_with_timestamp.write \
        .mode("overwrite") \
        .option("compression", "snappy") \
        .parquet(output_path)
    
    print(f"âœ… Metadatos de pelÃ­culas guardados en: {output_path}")
    print(f"ğŸ“Š Registros procesados: {movies_df_with_timestamp.count()}")
    
    return movies_df_with_timestamp

def ingest_credits(spark):
    """
    Ingesta los crÃ©ditos de pelÃ­culas desde CSV hacia la zona RAW.
    
    Args:
        spark (SparkSession): SesiÃ³n de Spark
    """
    print("ğŸ¬ Procesando crÃ©ditos de pelÃ­culas...")
    
    # Leer archivo CSV de crÃ©ditos
    credits_df = spark.read.csv(
        "credits.csv",
        header=True,
        inferSchema=True,
        escape='"',
        multiLine=True
    )
    
    # AÃ±adir columna de fecha de ingesta para trazabilidad
    credits_df_with_timestamp = credits_df.withColumn(
        "ingestion_date", 
        current_timestamp()
    )
    
    # Guardar en zona RAW como Parquet con compresiÃ³n snappy
    output_path = "datalake/raw/credits/"
    credits_df_with_timestamp.write \
        .mode("overwrite") \
        .option("compression", "snappy") \
        .parquet(output_path)
    
    print(f"âœ… CrÃ©ditos guardados en: {output_path}")
    print(f"ğŸ“Š Registros procesados: {credits_df_with_timestamp.count()}")
    
    return credits_df_with_timestamp

def main():
    """
    FunciÃ³n principal que ejecuta el pipeline de ingesta a la zona RAW.
    """
    print("ğŸš€ Iniciando Pipeline 1: Ingesta a zona RAW")
    print("=" * 50)
    
    # Crear sesiÃ³n de Spark
    spark = create_spark_session()
    
    try:
        # Crear directorios de salida si no existen
        os.makedirs("datalake/raw", exist_ok=True)
        
        # Ingestar metadatos de pelÃ­culas
        movies_df = ingest_movies_metadata(spark)
        
        # Ingestar crÃ©ditos
        credits_df = ingest_credits(spark)
        
        print("\nğŸ‰ Pipeline completado exitosamente!")
        print("ğŸ“ Datos guardados en zona RAW del Data Lake")
        print("ğŸ’¾ Formato: Parquet con compresiÃ³n snappy")
        print("ğŸ”„ Listos para ser procesados en la siguiente etapa (Refined)")
        
    except Exception as e:
        print(f"âŒ Error durante la ingesta: {str(e)}")
        raise
    
    finally:
        # Cerrar sesiÃ³n de Spark
        spark.stop()

if __name__ == "__main__":
    main() 